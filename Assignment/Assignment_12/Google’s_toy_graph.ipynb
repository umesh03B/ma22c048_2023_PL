{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uHFS7TfUB5V8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import urllib\n",
        "import gzip\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "# Download the Google Web Graph data\n",
        "url = \"https://snap.stanford.edu/data/web-Google.txt.gz\"\n",
        "data_path = 'web-Google.txt.gz'\n",
        "\n",
        "if not os.path.exists(data_path):\n",
        "    with urllib.request.urlopen(url) as response, open(data_path, 'wb') as out_file:\n",
        "        shutil.copyfileobj(response, out_file)\n",
        "\n",
        "# Function to load the graph from the file\n",
        "def load_graph(file_path):\n",
        "    edges = []\n",
        "    with gzip.open(file_path, 'rt', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            if line.startswith('#'):\n",
        "                continue\n",
        "            src, dest = map(int, line.strip().split())\n",
        "            edges.append((src, dest))\n",
        "    return edges\n",
        "\n",
        "# Load the Google Web Graph\n",
        "graph_edges = load_graph(data_path)\n",
        "\n",
        "# Function to compute PageRanks using the power method\n",
        "def power_method(graph, num_nodes, restart_prob=0.85, max_iterations=1000, tol=1e-6):\n",
        "    # Initialize PageRanks\n",
        "    pageranks = np.ones(num_nodes) / num_nodes\n",
        "    # Power iteration\n",
        "    for _ in range(max_iterations):\n",
        "        prev_pageranks = pageranks.copy()\n",
        "        pageranks = restart_prob * np.dot(graph, pageranks) + (1 - restart_prob) / num_nodes\n",
        "        # Check for convergence\n",
        "        if np.linalg.norm(pageranks - prev_pageranks, 1) < tol:\n",
        "            break\n",
        "    return pageranks\n",
        "\n",
        "# Get the number of nodes in the graph\n",
        "num_nodes = max(max(src, dest) for src, dest in graph_edges) + 1\n",
        "\n",
        "# Create the transition matrix\n",
        "transition_matrix = np.zeros((num_nodes, num_nodes))\n",
        "for src, dest in graph_edges:\n",
        "    transition_matrix[dest, src] = 1  # Reverse the direction for PageRank\n",
        "\n",
        "# Normalize the transition matrix\n",
        "transition_matrix /= transition_matrix.sum(axis=0)\n",
        "\n",
        "# Compute PageRanks\n",
        "pageranks = power_method(transition_matrix, num_nodes)\n",
        "\n",
        "# Display PageRanks for the first few nodes\n",
        "for node, pagerank in enumerate(pageranks[:10]):\n",
        "    print(f\"Node {node}: PageRank = {pagerank:.6f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib\n",
        "import shutil\n",
        "import numpy as np\n",
        "# The file is located here:\n",
        "url = \"https://snap.stanford.edu/data/web-Google.txt.gz\"\n",
        "\n",
        "# Download and copy it here using the code below:\n",
        "f = '../../data_external/web-Google.txt.gz'\n",
        "if not os.path.exists(f):\n",
        "  r = urllib.request.urlopen(url)\n",
        "  fo = open(f, 'wb')\n",
        "  shutil.copyfileobj(r, fo)\n",
        "  fo.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 245
        },
        "id": "l9OPqexcC6Tm",
        "outputId": "2fb7090c-85b7-4f25-c713-0f6daafc81f1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-3-553d1b7708fb>\u001b[0m in \u001b[0;36m<cell line: 10>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m   \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0murllib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0murlopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m   \u001b[0mfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'wb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m   \u001b[0mshutil\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcopyfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   \u001b[0mfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../../data_external/web-Google.txt.gz'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import urllib\n",
        "import shutil\n",
        "import numpy as np\n",
        "\n",
        "# Specify the directory for the file\n",
        "directory = '../../data_external/'\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "os.makedirs(directory, exist_ok=True)\n",
        "\n",
        "# Specify the file path\n",
        "file_path = os.path.join(directory, 'web-Google.txt.gz')\n",
        "\n",
        "# Download and copy the file\n",
        "if not os.path.exists(file_path):\n",
        "    url = \"https://snap.stanford.edu/data/web-Google.txt.gz\"\n",
        "    with urllib.request.urlopen(url) as response, open(file_path, 'wb') as out_file:\n",
        "        shutil.copyfileobj(response, out_file)\n",
        "\n",
        "\n",
        "# Now proceed with the rest of the code\n"
      ],
      "metadata": {
        "id": "vWSE-ZJoDb2L"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gzip\n",
        "import numpy as np\n",
        "from scipy.sparse import csr_matrix\n",
        "\n",
        "# Function to load the graph from the file\n",
        "def load_graph(file_path):\n",
        "    edges = []\n",
        "    with gzip.open(file_path, 'rt', encoding='utf-8') as file:\n",
        "        for line in file:\n",
        "            if line.startswith('#'):\n",
        "                continue\n",
        "            src, dest = map(int, line.strip().split())\n",
        "            edges.append((src, dest))\n",
        "    return edges\n",
        "\n",
        "# Function to compute PageRanks using the power method\n",
        "def power_method(graph, num_nodes, restart_prob=0.85, max_iterations=1000, tol=1e-6):\n",
        "    # Initialize PageRanks\n",
        "    pageranks = np.ones(num_nodes) / num_nodes\n",
        "    # Power iteration\n",
        "    for _ in range(max_iterations):\n",
        "        prev_pageranks = pageranks.copy()\n",
        "        pageranks = restart_prob * graph.dot(pageranks) + (1 - restart_prob) / num_nodes\n",
        "        # Check for convergence\n",
        "        if np.linalg.norm(pageranks - prev_pageranks, 1) < tol:\n",
        "            break\n",
        "    return pageranks\n",
        "\n",
        "# Load the Google Web Graph from the downloaded file\n",
        "data_path = '../../data_external/web-Google.txt.gz'\n",
        "\n",
        "# Load the Google Web Graph\n",
        "graph_edges = load_graph(data_path)\n",
        "\n",
        "# Get the number of nodes in the graph\n",
        "num_nodes = max(max(src, dest) for src, dest in graph_edges) + 1\n",
        "\n",
        "# Create the transition matrix using a sparse representation\n",
        "rows, cols = zip(*graph_edges)\n",
        "transition_matrix = csr_matrix((np.ones(len(rows)), (cols, rows)), shape=(num_nodes, num_nodes))\n",
        "\n",
        "# Compute PageRanks\n",
        "pageranks = power_method(transition_matrix, num_nodes)\n",
        "\n",
        "# Display PageRanks for the first few nodes\n",
        "for node, pagerank in enumerate(pageranks[:10]):\n",
        "    print(f\"Node {node}: PageRank = {pagerank:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HkLVLt5pEGNq",
        "outputId": "0d3a6460-272e-4413-89e7-e644a80783ba"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/numpy/linalg/linalg.py:2553: RuntimeWarning: overflow encountered in reduce\n",
            "  return add.reduce(abs(x), axis=axis, keepdims=keepdims)\n",
            "<ipython-input-2-0ba50a349c2a>:25: RuntimeWarning: invalid value encountered in subtract\n",
            "  if np.linalg.norm(pageranks - prev_pageranks, 1) < tol:\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Node 0: PageRank = inf\n",
            "Node 1: PageRank = inf\n",
            "Node 2: PageRank = inf\n",
            "Node 3: PageRank = inf\n",
            "Node 4: PageRank = inf\n",
            "Node 5: PageRank = inf\n",
            "Node 6: PageRank = 0.000000\n",
            "Node 7: PageRank = inf\n",
            "Node 8: PageRank = 0.000001\n",
            "Node 9: PageRank = 0.000000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "tPlN5fiPF34h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to compute PageRanks using the power method\n",
        "def power_method(graph, num_nodes, restart_prob=0.70, max_iterations=900, tol=1e-6):\n",
        "    # Initialize PageRanks with a non-uniform distribution\n",
        "    pageranks = np.ones(num_nodes) / num_nodes\n",
        "    # Power iteration\n",
        "    for iteration in range(1, max_iterations + 1):\n",
        "        prev_pageranks = pageranks.copy()\n",
        "        # Update PageRanks\n",
        "        pageranks = restart_prob * graph.dot(pageranks) + (1 - restart_prob) / num_nodes\n",
        "        # Check for convergence\n",
        "        diff = np.linalg.norm(pageranks - prev_pageranks, 1)\n",
        "        if diff < tol:\n",
        "            print(f\"Converged after {iteration} iterations with a difference of {diff:.6f}\")\n",
        "            break\n",
        "    else:\n",
        "        print(\"Did not converge within the maximum number of iterations\")\n",
        "\n",
        "    return pageranks\n",
        "\n",
        "# Compute PageRanks\n",
        "pageranks = power_method(transition_matrix, num_nodes)\n",
        "\n",
        "# Display PageRanks for the first few nodes\n",
        "for node, pagerank in enumerate(pageranks[:10]):\n",
        "    print(f\"Node {node}: PageRank = {pagerank:.6f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aMcrU5t-CsiX",
        "outputId": "412e4d7c-aa22-45c9-f31d-6ddecb06445c"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-4-8001d350d211>:11: RuntimeWarning: invalid value encountered in subtract\n",
            "  diff = np.linalg.norm(pageranks - prev_pageranks, 1)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Did not converge within the maximum number of iterations\n",
            "Node 0: PageRank = inf\n",
            "Node 1: PageRank = inf\n",
            "Node 2: PageRank = inf\n",
            "Node 3: PageRank = inf\n",
            "Node 4: PageRank = inf\n",
            "Node 5: PageRank = inf\n",
            "Node 6: PageRank = 0.000000\n",
            "Node 7: PageRank = inf\n",
            "Node 8: PageRank = 0.000001\n",
            "Node 9: PageRank = 0.000001\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "XDBKhhxNEiS2"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}